{"cells":[{"source":"<p align=\"center\" width=\"100%\">\n    <img width=\"40%\" src=\"customer_support_icon.JPG\"> \n</p>\n\nA retail company is on a transformative journey, aiming to elevate their customer services through cutting-edge advancements in Speech Recognition and Natural Language Processing (NLP). As the machine learning engineer for this initiative, you are tasked with developing functionalities that not only convert customer support audio calls into text but also explore methodologies to extract insights from transcribed texts.\n\nIn this dynamic project, we leverage the power of `SpeechRecognition`, `Pydub`, and `spaCy` – three open-source packages that form the backbone of your solution. Your objectives are:\n  - Transcribe a sample customer audio call, stored at `sample_customer_call.wav`, to showcase the power of open-source speech recognition technology.\n  - Analyze sentiment, identify common named entities, and enhance user experience by searching for the most similar customer calls based on a given query from a subset of their pre-transcribed call data, stored at `customer_call_transcriptions.csv`.\n\nThis project is an opportunity to unlock the potential of machine learning to revolutionize customer support. Let's delve into the interplay between technology and service excellence.","metadata":{},"id":"d5e81b43-ccfd-4fc6-902c-59cd49aa9913","cell_type":"markdown"},{"source":"!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm","metadata":{"outputsMetadata":{"0":{"height":488,"type":"stream"}},"executionCancelledAt":null,"executionTime":18501,"lastExecutedAt":1724776405122,"lastExecutedByKernel":"0d4a5ac6-886e-4187-af6f-0fc2fad11d9f","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install SpeechRecognition\n!pip install pydub\n!pip install spacy\n!python3 -m spacy download en_core_web_sm"},"id":"d0f1598e-18a8-45d5-8387-bf2f5ce4ffd6","cell_type":"code","execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: SpeechRecognition in /home/repl/.local/lib/python3.8/site-packages (3.10.4)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from SpeechRecognition) (2.31.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from SpeechRecognition) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->SpeechRecognition) (2019.11.28)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pydub in /home/repl/.local/lib/python3.8/site-packages (0.25.1)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.0)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.23.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.1.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (65.6.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.1.1)\nDefaulting to user installation because normal site-packages is not writeable\nCollecting en-core-web-sm==3.6.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m150.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.0)\nRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.2)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2019.11.28)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n"}]},{"source":"# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1724776405178,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport pandas as pd\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport speech_recognition as sr\nfrom pydub import AudioSegment\n\nimport spacy","outputsMetadata":{"0":{"height":77,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"0d4a5ac6-886e-4187-af6f-0fc2fad11d9f"},"id":"d6f3dd61-8c75-48d4-b2a5-79cd0b444ddb","cell_type":"code","execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/repl/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"}]},{"source":"# Start coding here\n# 1 # Implement speech recognition and calculate audio statistics\n\ndef transcribe_audio(filename):\n  # Setup a recognizer instance\n  recognizer = sr.Recognizer()\n  \n  # Import the audio file and convert to audio data\n  audio_file = sr.AudioFile(filename)\n  with audio_file as source:\n    audio_data = recognizer.record(source)\n  \n  # Return the transcribed text\n  return recognizer.recognize_google(audio_data)\n\n# Transcribe AudioData to text\ntranscribed_text = transcribe_audio(\"sample_customer_call.wav\")\nprint(transcribed_text)","metadata":{"executionCancelledAt":null,"executionTime":1407,"lastExecutedAt":1724776406585,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# 1 # Implement speech recognition and calculate audio statistics\n\ndef transcribe_audio(filename):\n  # Setup a recognizer instance\n  recognizer = sr.Recognizer()\n  \n  # Import the audio file and convert to audio data\n  audio_file = sr.AudioFile(filename)\n  with audio_file as source:\n    audio_data = recognizer.record(source)\n  \n  # Return the transcribed text\n  return recognizer.recognize_google(audio_data)\n\n# Transcribe AudioData to text\ntranscribed_text = transcribe_audio(\"sample_customer_call.wav\")\nprint(transcribed_text)","outputsMetadata":{"0":{"height":38,"type":"stream"}},"lastExecutedByKernel":"0d4a5ac6-886e-4187-af6f-0fc2fad11d9f"},"id":"250524c2-1bd3-4ff8-a224-8fa007566c1b","cell_type":"code","execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":"hello I'm experiencing an issue with your product I'd like to speak to someone about a replacement\n"}]},{"source":"# Import audio file\nwav_file = AudioSegment.from_file(file=\"sample_customer_call.wav\")\n\n# Find stats \nframe_rate = wav_file.frame_rate\nprint(f\"Frame Rate: {frame_rate}\")\nnumber_channels = wav_file.channels \nprint(f\"Channels: {number_channels}\")","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1724776406634,"lastExecutedByKernel":"0d4a5ac6-886e-4187-af6f-0fc2fad11d9f","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import audio file\nwav_file = AudioSegment.from_file(file=\"sample_customer_call.wav\")\n\n# Find stats \nframe_rate = wav_file.frame_rate\nprint(f\"Frame Rate: {frame_rate}\")\nnumber_channels = wav_file.channels \nprint(f\"Channels: {number_channels}\")","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"47662585-a2d8-4c2a-95f4-b563f754b7b7","outputs":[{"output_type":"stream","name":"stdout","text":"Frame Rate: 44100\nChannels: 1\n"}],"execution_count":41},{"source":"# 2 # Perform sentiment analysis\n\n# Load the CSV file\ndf = pd.read_csv(\"customer_call_transcriptions.csv\")\n\n# Initialize the sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Initialize the prediction column\ndf[\"prediction\"] = \"\"\n\n# Iterate over each row in the DataFrame\nfor index, row in df.iterrows():\n    # Get the text to analyze\n    text = row['text']\n    \n    # Calculate the polarity scores\n    scores = sid.polarity_scores(text)\n    compound_score = scores['compound']\n    \n    # Determine the sentiment based on the compound score\n    if compound_score >= 0.05:\n        sentiment = \"positive\"\n    elif compound_score <= -0.05:\n        sentiment = \"negative\"\n    else:\n        sentiment = \"neutral\"\n    \n    # Assign the sentiment to the appropriate row in the DataFrame\n    df.at[index, \"prediction\"] = sentiment\n\n# Print the first few rows of the DataFrame\nprint(df.head())\n\ntrue_positive = len(df[(df['sentiment_label'] == \"positive\") & (df['prediction'] == \"positive\")])\nprint(true_positive)","metadata":{"executionCancelledAt":null,"executionTime":68,"lastExecutedAt":1724776406702,"lastExecutedByKernel":"0d4a5ac6-886e-4187-af6f-0fc2fad11d9f","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 2 # Perform sentiment analysis\n\n# Load the CSV file\ndf = pd.read_csv(\"customer_call_transcriptions.csv\")\n\n# Initialize the sentiment analyzer\nsid = SentimentIntensityAnalyzer()\n\n# Initialize the prediction column\ndf[\"prediction\"] = \"\"\n\n# Iterate over each row in the DataFrame\nfor index, row in df.iterrows():\n    # Get the text to analyze\n    text = row['text']\n    \n    # Calculate the polarity scores\n    scores = sid.polarity_scores(text)\n    compound_score = scores['compound']\n    \n    # Determine the sentiment based on the compound score\n    if compound_score >= 0.05:\n        sentiment = \"positive\"\n    elif compound_score <= -0.05:\n        sentiment = \"negative\"\n    else:\n        sentiment = \"neutral\"\n    \n    # Assign the sentiment to the appropriate row in the DataFrame\n    df.at[index, \"prediction\"] = sentiment\n\n# Print the first few rows of the DataFrame\nprint(df.head())\n\ntrue_positive = len(df[(df['sentiment_label'] == \"positive\") & (df['prediction'] == \"positive\")])\nprint(true_positive)","outputsMetadata":{"0":{"height":164,"type":"stream"}}},"cell_type":"code","id":"a09bb7d2-7d45-447a-ab72-8484bf7cae4a","outputs":[{"output_type":"stream","name":"stdout","text":"                                                text sentiment_label prediction\n0  how's it going Arthur I just placed an order w...        negative   negative\n1  yeah hello I'm just wondering if I can speak t...         neutral   positive\n2  hey I receive my order but it's the wrong size...        negative   negative\n3  hi David I just placed an order online and I w...         neutral    neutral\n4  hey I bought something from your website the o...        negative    neutral\n2\n"}],"execution_count":42},{"source":"# 3 # Run named entity recognition\n\nfrom collections import Counter\n\nnlp = spacy.load(\"en_core_web_sm\")\nall_entities = []\n\n# Iterate over each transcription\nfor text in df['text']:\n    # Process the text with spaCy to extract named entities\n    doc = nlp(text)\n    # Add entities to the list\n    all_entities.extend([ent.text for ent in doc.ents])\n\n# Count the frequency of each named entity\nentity_counts = Counter(all_entities)\n\n# Find the most common named entity\nmost_common_entity = entity_counts.most_common(1)\nmost_freq_ent = most_common_entity[0][0]\nprint(f\"The most frequently mentioned entity is: {most_freq_ent} with {most_common_entity[0][1]} mentions.\")","metadata":{"executionCancelledAt":null,"executionTime":963,"lastExecutedAt":1724776407665,"lastExecutedByKernel":"0d4a5ac6-886e-4187-af6f-0fc2fad11d9f","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 3 # Run named entity recognition\n\nfrom collections import Counter\n\nnlp = spacy.load(\"en_core_web_sm\")\nall_entities = []\n\n# Iterate over each transcription\nfor text in df['text']:\n    # Process the text with spaCy to extract named entities\n    doc = nlp(text)\n    # Add entities to the list\n    all_entities.extend([ent.text for ent in doc.ents])\n\n# Count the frequency of each named entity\nentity_counts = Counter(all_entities)\n\n# Find the most common named entity\nmost_common_entity = entity_counts.most_common(1)\nmost_freq_ent = most_common_entity[0][0]\nprint(f\"The most frequently mentioned entity is: {most_freq_ent} with {most_common_entity[0][1]} mentions.\")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"480996c5-cd75-4e49-a6fa-c6e788bc0dc3","outputs":[{"output_type":"stream","name":"stdout","text":"The most frequently mentioned entity is: yesterday with 15 mentions.\n"}],"execution_count":43},{"source":"# 4 # Find most similar texts\n\n# Create a documents list containing Doc containers\ndocuments = [nlp(t) for t in df['text']]\n# Create a Doc container of the query\nquery = \"wrong package delivery\"\nquery_document = nlp(query)\n\nsimilarity_scores = []\n\nfor doc in documents:\n    # Calculate similarity\n    similarity = query_document.similarity(doc)\n    # Store the similarity score\n    similarity_scores.append(similarity)\n    \n# Add similarity scores to the DataFrame\ndf['similarity'] = similarity_scores\n\n# Find the most similar document\nmost_similar_doc = df.loc[df['similarity'].idxmax()]\n\nmost_similar_text = most_similar_doc['text']\n# Print the most similar document and its similarity score\nprint(f\"Most similar document: {most_similar_text}\")\nprint(f\"Similarity score: {most_similar_doc['similarity']}\")","metadata":{"executionCancelledAt":null,"executionTime":552,"lastExecutedAt":1724776408217,"lastExecutedByKernel":"0d4a5ac6-886e-4187-af6f-0fc2fad11d9f","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 4 # Find most similar texts\n\n# Create a documents list containing Doc containers\ndocuments = [nlp(t) for t in df['text']]\n# Create a Doc container of the query\nquery = \"wrong package delivery\"\nquery_document = nlp(query)\n\nsimilarity_scores = []\n\nfor doc in documents:\n    # Calculate similarity\n    similarity = query_document.similarity(doc)\n    # Store the similarity score\n    similarity_scores.append(similarity)\n    \n# Add similarity scores to the DataFrame\ndf['similarity'] = similarity_scores\n\n# Find the most similar document\nmost_similar_doc = df.loc[df['similarity'].idxmax()]\n\nmost_similar_text = most_similar_doc['text']\n# Print the most similar document and its similarity score\nprint(f\"Most similar document: {most_similar_text}\")\nprint(f\"Similarity score: {most_similar_doc['similarity']}\")","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"4f7a4c00-61ed-4728-a431-a72cca85001b","outputs":[{"output_type":"stream","name":"stdout","text":"Most similar document: wrong package delivered\nSimilarity score: 0.5110670022005256\n"}],"execution_count":44}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}